{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance and Spelling Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download this dataset in the same folder : http://www.gutenberg.org/files/140/140-0.txt \n",
    "In this tutorial we will learn how to correct non word spelling errors. This tutorial is divided into many parts.\n",
    "In each part we will learn some theory and then we will try to write the code. I have provided the sample codes in \n",
    "Python 2.7 but you choose any language which you wish to choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will proceed step by step. First of all it is important to understand what is the meaning of distance between two words. There can various various notions of distance between two words. However in this article we will only consider the most common notion of distance between two words that is edit distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intution behind edit distance is to find minimum number of edits which we have to do in first word in order to reach second word. Here we are taking 3 valid edits namely Insertion, Deletion and Substitution. All the edits are specified same cost of one. \n",
    "One of the naive approch for this would be to search for all the paths through which we can reach from word1 to word2. But this approach is not optimal because the number of possibilities would be exponential. \n",
    "This can be solved using Dynamic programming(DP) approach. The idea behind DP is that when going from word1 to word2 we can either do an insertion, deletion or substitution. \n",
    "Below is the implementation of edit distance between two words but I strongly recomment you to first try doing it on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Dynamic Programming based Python program for edit \n",
    "# This code has been taken from geeksforgeeks\n",
    "def editDistDP(str1, str2): \n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "\n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0: # insert all the characters \n",
    "                dp[i][j] = j \n",
    "            elif j == 0:# delete all characters \n",
    "                dp[i][j] = i\n",
    "            elif str1[i-1] == str2[j-1]:# if last characters are same then we just remove ending characters \n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else: \n",
    "                dp[i][j] = 1 + min(dp[i][j-1], # Insert \n",
    "                                dp[i-1][j],\t # Remove \n",
    "                                dp[i-1][j-1]) # Replace \n",
    "\n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Driver program \n",
    "str1 = \"peace\"\n",
    "str2 = \"piece\"\n",
    "print(editDistDP(str1, str2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! We are done with first part. But there is some problem with this approach. Can you guess what is the problem? So the problem with this approach is that it weights all the edits to same value. But in real life generally people tend to make mistakes between letter \"i\" and \"o\" or when we are typing it is common to type adjacent key instead of actual key. Thus the distance assigned to them should be less. This leads us to our next algorithm i.e. weighted edit distance. \n",
    "Given the cost value of all the inserts, deletes and substitutions write a code to find weighted edit distance between two words. \n",
    "Code is given below but I strongly recommend you to first try it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del_cost,ins_cost are array of size 26 which contains deletion and insertion costs.\n",
    "# sub_cost,ins_cost is array of size 26*26 which contains substitution cost of two words.\n",
    "\n",
    "def weightededitDistDP(str1, str2,del_cost,ins_cost,sub_cost):\n",
    "    m = len(str1)\n",
    "    n = len(str2)\n",
    "    # Create a table to store results of subproblems \n",
    "    dp = [[0 for x in range(n+1)] for x in range(m+1)] \n",
    "\n",
    "    # Fill d[][] in bottom up manner \n",
    "    for i in range(m+1): \n",
    "        for j in range(n+1): \n",
    "            if i == 0: # insert all the characters \n",
    "                dp[i][j] = j \n",
    "            elif j == 0:# delete all characters \n",
    "                dp[i][j] = i\n",
    "            elif str1[i-1] == str2[j-1]:# if last characters are same then we just remove ending characters \n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "             \n",
    "            else: \n",
    "                try:\n",
    "                    dp[i][j] = min(dp[i][j-1]+ins_cost[ord(str2[j])-97],\t # Insert \n",
    "                                    dp[i-1][j]+del_cost[ord(str1[i])-97],\t # Remove \n",
    "                                    dp[i-1][j-1]+sub_cost[ord(str2[j])-97][ord(str1[i])-97]) # Replace \n",
    "                except:\n",
    "                    pass\n",
    "    return dp[m][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Driver program \n",
    "str1 = \"sunday\"\n",
    "str2 = \"saturday\"\n",
    "del_cost = [1]*26\n",
    "ins_cost = [1]*26\n",
    "sub_cost = [[1]*26]*26\n",
    "print(weightededitDistDP(str1, str2,del_cost,ins_cost,sub_cost)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example I have taken the all the cost to be one but you can change its value also. \n",
    "Next we move to the problem of spelling correction. So the problem is to estimate the correct spelling of the word given wrong spelling of that word(not present in vocabulary).\n",
    "Code is given for this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def nearest_word(word,vocab):\n",
    "    mini = 100000\n",
    "    for i in vocab:\n",
    "        if(editDistDP(word,i)<mini):\n",
    "            nn = i\n",
    "            mini = editDistDP(word,i)\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : problem of wrong speling words\n",
      "Correct Word : problem of wrong spelling words\n"
     ]
    }
   ],
   "source": [
    "data = 'Good job! We are done with first part. But there is some problem with this approach. Can you guess what is the problem? So the problem with this approach is that it weights all the edits to same value. But in real life generally people tend to make mistakes between letter \"i\" and \"o\" or when we are typing it is common to type adjacent key instead of actual key. Thus the distance assigned to them should be less. This leads us to our next algorithm i.e. weighted edit distance. Given the cost value of all the inserts, deletes and substitutions write a code to find weighted edit distance between two words. Code is given below but I strongly recommend you to first try it yourself In the above example I have taken the all the cost to be one but you can change its value also. Congratulations on completing the second part of this tutorial. Next we move to the problem of spelling correction. So the problem is to estimate the correct spelling of the word given wrong spelling of that word. In this part we are assuming that the wrong spelling words are not present in the vocabulary. We will relax this constraint later in the tutorial. So the naive algorithm would be to check edit distance with the whole set of words in the vocabulary if we find a word with wrong spelling(word which is not present in vocabulary). Code is given for this below but you should first try it youself.'\n",
    "data = data.lower()\n",
    "vocab = str.split(data)\n",
    "wrong_sent = 'problem of wrong speling words'\n",
    "print 'Input :',wrong_sent\n",
    "sentence = str.split(wrong_sent)\n",
    "print 'Correct Word :',\n",
    "for i in sentence:\n",
    "    if(i not in vocab):\n",
    "        print nearest_word(i,vocab),\n",
    "    else:\n",
    "        print i,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code I have made a small dataset and constructed a toy example. The main idea is to understand the concept rather than worring about the intricacies of the vocabulary.\n",
    "This algorithm is correct but it is not efficient as we have to check all the edit distance of all the words in the vocabulary with the current word. Can you devise a better approach for this problem??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple approach is to generate all the words which are having an edit distance less than three and check if those words are there in the vocabulary or not. It is based on the common observation that most of the wrong spellings have one or two wrong letters. This would decrease our computation by a large extent. \n",
    "Now your next task is to write a function which return all the words(might not be in vocabulary also)which are at an edit distance of one and two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def words_range(word):\n",
    "    dic = set()\n",
    "    letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "    \n",
    "    word = list(word)\n",
    "    # deletion words\n",
    "    for i in range(len(word)):\n",
    "        \n",
    "        curr_word = copy.deepcopy(word)\n",
    "        del curr_word[i]\n",
    "        dic.add(''.join(curr_word))\n",
    "    \n",
    "    \n",
    "    # insertion words\n",
    "    for i in range(len(word)):\n",
    "        for j in letters:\n",
    "            curr_word = copy.deepcopy(word)\n",
    "            curr_word.insert(i, j)\n",
    "            dic.add(''.join(curr_word))\n",
    "    \n",
    "    \n",
    "    # subsitution words\n",
    "    for i in range(len(word)):\n",
    "        for j in letters:\n",
    "            curr_word = copy.deepcopy(word)\n",
    "            curr_word[i]=j\n",
    "            dic.add(''.join(curr_word))\n",
    "    return dic\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code finds all the words which are at edit distance 1 from the input word. Now we have to search all these words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : congratualations\n",
      "Output : congratulations\n"
     ]
    }
   ],
   "source": [
    "s = 'congratualations'\n",
    "print 'Input :',s\n",
    "print 'Output :',\n",
    "near_words = words_range(s)\n",
    "for i in near_words:\n",
    "    if i in vocab:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Job! Now we can move to next phase of the tutorial. However the above algorithm has reduced our search space but still the search space is very large. This number is acceptable for languages like english but with languages like chinese whose size of alphabet is very large, this algorithm is not efficient. We can not spend this much amount of time for checking wrong spellings. A better alternative is to store some amount of information offline which can reduce the runtime.\n",
    "This leads us to our next model.\n",
    "So the idea is that instead of doing all the edit operations online, we can do only delete operations online and store all the words which are at delete distance of 1 from the words in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_words(word):\n",
    "    dic = set()\n",
    "    word = list(word)\n",
    "    # deletion words\n",
    "    for i in range(len(word)):\n",
    "        \n",
    "        curr_word = copy.deepcopy(word)\n",
    "        del curr_word[i]\n",
    "        dic.add(''.join(curr_word))\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_dic = {}\n",
    "for i in vocab:\n",
    "    delete_dic[i] =list(delete_words(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong word : secend\n",
      "Correct word : second\n"
     ]
    }
   ],
   "source": [
    "wrong_word = 'secend'\n",
    "print 'Wrong word :',wrong_word\n",
    "print 'Correct word :',\n",
    "wrong_word_delete = list(delete_words(wrong_word))\n",
    "for i in wrong_word_delete:\n",
    "    for j in delete_dic.keys():\n",
    "        if(i in delete_dic[j]):\n",
    "            print j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best which we can do in order to find correct word from an incorrect word. But still this approach is not full proof. Can you guess why? So the problem arises from the fact that erronious word can also be present in our vocabulary. For example if the someone writes \"three\" instead of \"there\" then our approach will fail as it would not detect any error as both the words are present in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus in order to find the correct word we need to need to understand the context in which that word has been used. We can understand the context of a word by seeing its adjacent words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import unicodedata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = open('140-0.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find the count of unigrams and bigrams that are there in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text.decode('utf-8'))\n",
    "train_data = []\n",
    "n = len(sentences)\n",
    "for i in range(n):\n",
    "    train_data.append('<'+unicodedata.normalize('NFKD', sentences[i]).encode('ascii','ignore')+'/>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram(Vocabulary)\n",
    "dic_uni = {}\n",
    "for i in train_data:\n",
    "    # word tokenization\n",
    "    words = word_tokenize(i)\n",
    "    for j in range(0,len(words)):\n",
    "        if(words[j] not in dic_uni):\n",
    "            dic_uni[words[j]] = 1\n",
    "        else:\n",
    "            dic_uni[words[j]] = dic_uni[words[j]] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE bigrams\n",
    "dic_bi = {}\n",
    "for i in train_data:\n",
    "    words = word_tokenize(i)\n",
    "    for j in range(0,len(words)-1):\n",
    "        if(' '.join(words[j:j+2]) not in dic_bi):\n",
    "            dic_bi[' '.join(words[j:j+2])] = 1\n",
    "        else:\n",
    "            dic_bi[' '.join(words[j:j+2])] = dic_bi[' '.join(words[j:j+2])] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let suppose that your friend wanted to type \"little peace of cake\". But instead he typed \"little piece of cake\". Now since both piece and peace are present in the corpus we would not be able to find the correct word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will assume that there is one error in a sentence. This assumption is generally valid because most of the time there is only one error in a sentence. Now for each word in the sentence we would check all the words which are at edit distance less than 2 from the given word. If that word is present in the vocabulary then we will consider it as an candidate word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will find the probability of that word coming with those adjacent words. I am assuming that a careful reader can find the candidates words of a particular word using th above mentioned codes. Now we can find the probability of that word using the bigram counts.\n",
    "                                P(A|B) = count(A.B)/count(B)\n",
    "Using the above relation we can find the correct word. \n",
    "I have provided the code for finding the correct word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_word(str1,str2):\n",
    "    #str1 and str2 are strings of length 3 in which middle word is ambiguous\n",
    "    lst1 = str1.split()\n",
    "    lst2 = str2.split()\n",
    "    try:# if count of bigram is 0\n",
    "        p1 = dic_bi[''.join(lst1[1:])]*dic_bi[''.join(lst1[:2])]\n",
    "        p1 = p1/(dic_uni[lst1[0]]*dic_uni[lst1[1]])\n",
    "        \n",
    "    except:\n",
    "        p1 = 0\n",
    "    try:\n",
    "        p2 = dic_bi[''.join(lst2[1:])]*dic_bi[''.join(lst2[:2])]\n",
    "        p2 = p2/(dic_uni[lst2[0]]*dic_uni[lst2[1]])\n",
    "        \n",
    "    except:\n",
    "        p2 = 0\n",
    "    if(p1>p2):\n",
    "        return ' '.join(lst1)\n",
    "    else:\n",
    "        return ' '.join(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct sentence is : little piece of\n"
     ]
    }
   ],
   "source": [
    "str1 = 'little peace of'\n",
    "str2 = 'little piece of'\n",
    "print 'correct sentence is :',correct_word(str1,str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
